{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDD_2022_tutorial_session3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gstoil/Hydrowl/blob/master/KDD_2022_tutorial_session3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "This notebook will walk you through the basic steps for knowledge distillation, using simple teacher and student models implemented from [Hugging Face Transformer](https://huggingface.co/docs/transformers/index).We will also conduct experiments on the QADSM task in [xGLUE](https://huggingface.co/datasets/xglue) dataset, which is extracted from real Bing Ads traffic. "
      ],
      "metadata": {
        "id": "RCHRwy30Kg4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/KDD_2022/\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "# test whether we can import packages properly\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "nTriABf_SSSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5eca77-e563-4b1e-81aa-90725d0dca0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/KDD_2022/'\n",
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 16.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, fsspec, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 fsspec-2022.7.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations for Colab\n",
        "\n",
        "Running the code snippets in this notebook requires a GPU runtime, and we also need to install some dependencies.\n"
      ],
      "metadata": {
        "id": "3wOHwbcmcWDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code used in this notebook are available in https://github.com/sufferandjoy/kdd_2022_tutorial.git:"
      ],
      "metadata": {
        "id": "xmG1lH31dPrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sufferandjoy/kdd_2022_tutorial.git"
      ],
      "metadata": {
        "id": "SiybH5wsdELS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9887926-9d6b-4cc2-c76c-8b6429344433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kdd_2022_tutorial'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 42 (delta 17), reused 18 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teacher and Student Model\n",
        "\n",
        "As mentioned above, both our teacher model and student model are implemented using Hugging Face Tranformer. More specifically, we use [BERT-Mini](https://huggingface.co/google/bert_uncased_L-4_H-256_A-4) (4 layers, 4 attention heads, hidden layer size 256) as our teacher model, and a [TwinBERT](https://arxiv.org/abs/2002.06275) model constructed from two [BERT-Tiny](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) (2 layers, 2 attention heads, hidden layer size 128) as student model. Other model structures are also supported, which could be set by the `teacher_pretrained` and `student_pretrained` parameter. Below are the model strcture specified in `model.py`:"
      ],
      "metadata": {
        "id": "-qKZi5dPdfsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.model = BertModel.from_pretrained(args.teacher_pretrained)\n",
        "        hidden_size = int(args.teacher_pretrained.split('/')[1].split('_')[3].split('-')[-1])\n",
        "        self.ff = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        bert_output = self.model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        output = torch.sigmoid(self.ff(bert_output.pooler_output))\n",
        "        return output\n",
        "\n",
        "\n",
        "class TwinBERT(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(TwinBERT, self).__init__()\n",
        "        self.encoder_model = BertModel.from_pretrained(args.student_pretrained)\n",
        "\n",
        "    def forward(self, seq1, mask1, seq2, mask2):\n",
        "        output_1 = self.encoder_model(seq1, attention_mask=mask1).pooler_output\n",
        "        output_2 = self.encoder_model(seq2, attention_mask=mask2).pooler_output\n",
        "        cosine_similarity = nn.functional.cosine_similarity(output_1, output_2).unsqueeze(-1)\n",
        "        return cosine_similarity"
      ],
      "metadata": {
        "id": "LS1_aPhrfdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TwinBERT model has a two-tower structure, implemented as two BERT encoders sharing the same weights, as shown in the figure below. In this notebook, we will use query as input to the left encoder, and the concatenation of ad_title and ad_description as input to the other encoder.\n",
        "\n",
        "![twinbert.png](https://drive.google.com/uc?id=1H3qpUI8LwqOKnk9NWbf7s6cY04SlbtED)\n"
      ],
      "metadata": {
        "id": "RuoK0T92gKVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "Below we show several samples from the QADSM task in XGLUE, where each sample contains `query`, `ad_title`, `ad_description`, and a binary label named `relevance_label` indicating the relevance between each query-ad pair. In this notebook we will conduct training on the 100K `train` split, and conduct evaluation on the 10K `test.en` split. We further remove all training and test samples starting with \"ERROR_AdRejected\".\n",
        "\n",
        "![QADSM task in XGLUE](https://drive.google.com/uc?id=1TQDw1b5iZeonUONcbdutO-7XkmFhwEva)\n",
        "\n",
        "The logic for loading this dataset and pre-processing samples are implemented in `utils.py`. Because our teacher model and student model have different input schema (teacher takes as input a single text sequence while student takes as input two), we implement two different preprocess functions as below. Note that in `preprocess_function_student()`, we will concatenate `ad_title` and `ad_description` as the ad text, and the output will contain tokenized results for both query and ad text."
      ],
      "metadata": {
        "id": "k84Hutflf5J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Concatenate ad_title and ad_description\n",
        "    texts = []\n",
        "    for i in range(len(examples['query'])):\n",
        "        new_text = (examples['query'][i], examples['ad_title'][i] + ' ' + examples['ad_description'][i])\n",
        "        texts.append(new_text)\n",
        "\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "\n",
        "def preprocess_function_student(examples):\n",
        "    # Concatenate ad_title and ad_description\n",
        "    texts = []\n",
        "    for i in range(len(examples['query'])):\n",
        "        new_text = examples['ad_title'][i] + ' ' + examples['ad_description'][i]\n",
        "        texts.append(new_text)\n",
        "\n",
        "    tok_q = tokenizer(\n",
        "        examples['query'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length_query,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "\n",
        "    tok_a = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=args.max_length_ad,\n",
        "        return_special_tokens_mask=False,\n",
        "    )\n",
        "    tok_q['input_ids_2'] = tok_a['input_ids']\n",
        "    tok_q['attention_mask_2'] = tok_a['attention_mask']\n",
        "    tok_q['token_type_ids_2'] = tok_a['token_type_ids']\n",
        "    return tok_q\n",
        "\n",
        "# process dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function if args.model == 'teacher' else preprocess_function_student,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    # remove_columns=dataset[\"train\"].column_names,\n",
        "    load_from_cache_file=True,\n",
        "    # desc=\"Running tokenizer on dataset line_by_line\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZOfmoBvjqOis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "c21cf1c7-5b3b-4e5a-b906-b42a518360aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f3db65dd6a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# process dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m tokenized_dataset = dataset.map(\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mpreprocess_function\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'teacher'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpreprocess_function_student\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Experiment on Knowledge Distillation\n",
        "\n",
        "Having introduced both models and dataset, next we will show you the major steps in knowledge distillation and demonstrate its effectiveness. To begin with, we will first introduce a key parameter in our code called `task`, which supports five different settings:\n",
        "\n",
        "\n",
        "*   `teacher_ft`: this is the setting that allows us to load a pretrained teacher model and finetune it on the binary labels in QADSM task.\n",
        "*   `student_ft`: similarly, this is the setting to finetune our student model directly on the binary labels.\n",
        "*   `teacher_inf`: this setting allows us to conduct inference using our best finetuned teacher model, where \"best\" means having the smallest validation loss.\n",
        "*   `student_kd`: this setting allows us to train our student model by regression to the teacher model obtained in `teacher_inf` setting.\n",
        "*   `eval`: this setting will do a full evaluation on the same test data, to compare the performance of `teacher_ft`, `student_ft` and `student_kd`.\n",
        "\n",
        "### Step 0: Student Finetuning on Binary Labels as Baseline\n",
        "\n",
        "We firstly run our code using the `student_ft` setting, in order to get an idea on how well we are doing without knowledge distillation:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFZ916dLrTqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task student_ft --train_batch_size 512 --val_batch_size 2048"
      ],
      "metadata": {
        "id": "5CSXy0Dgdh8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Teacher Finetuning\n",
        "\n",
        "Having obtained the above baseline, we then turn to the first step in knowledge distillation which is teacher finetuning. Typical teacher models used in industrial applications are usually very powerful and hence very resource-consuming, but thanks to knowldege distillation, we do not need to worry about how to serve such models online. Instead, all the training and inference jobs running on these models would happen offline only, and it is the light-weight student model that would be deployed to the online environment. \n",
        "\n",
        "In this notebook, we can run the following code snippet for teacher finetuning, similar to what we did in the last step:"
      ],
      "metadata": {
        "id": "-fWzvrIpYqZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task teacher_ft --train_batch_size 512 --val_batch_size 2048"
      ],
      "metadata": {
        "id": "a39AbzejAKEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Teacher Inference\n",
        "\n",
        "Once the `teacher_ft` task completed, we can then inference the entire training corpus to get teacher score on each training sample. The data set to be inferenced in this step is often refered to as **distillation data**, and it does not need to have human labels. That is why we can often leverage business logs in industrial scenairos, since we usually have plenty of business logs and sampling from these logs is much easier and cheaper than labeling by human judges.\n",
        "\n",
        "Here we will do the inference on the same 100K training data used in the above finetuning steps. The scale of this data is much smaller than what we typically have in industrial scenarios (where we can sample billions of logs), but as we will see later, this facilitates a fair comparison between `student_ft` and `student_kd`:"
      ],
      "metadata": {
        "id": "vp__hcF8dsC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task teacher_inf --val_batch_size 4096"
      ],
      "metadata": {
        "id": "5go8Z5z6K63k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation would output a prediction.tsv file under `output/teacher_inf`. We need to copy this file to `data/QADSM/`, since this is where `load_dataset.py` would try to load the inferenced data in the next step."
      ],
      "metadata": {
        "id": "I_wHOjxuxNOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('data/QADSM'):\n",
        "  os.makedirs('data/QADSM')\n",
        "!cp output/teacher_inf/prediction.tsv data/QADSM/prediction.tsv"
      ],
      "metadata": {
        "id": "tO9CzERNOdSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Distill Knowledge from Teacher to Student\n",
        "\n",
        "Finally it comes to the real distillation step! All we need to do is to run the code snippet once again with task `student_kd`. Note that this time we need to specify the relative path of `load_dataset.py` using the `load_dataset_py_path` parameter:"
      ],
      "metadata": {
        "id": "Kd-jjtD2eN9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task student_kd --train_batch_size 512 --val_batch_size 2048 --load_dataset_py_path kdd_2022_tutorial/load_dataset.py"
      ],
      "metadata": {
        "id": "QrmMnLYZPGlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Now that all the major steps for knowledge distillaiton have completed, we want to know whether all these efforts have brought any real impact. To see this, let us run our script for the last time with task `eval`, which will load the best checkpoint under `teacher_ft`, `student_ft` and `student_kd` settings respectively and conduct evaluation on the same test data.\n",
        "\n",
        "The would print a table in the end of its log file, where the last row highlights the improvement by comparing metrics for `student_kd` against that of `student_ft`. As we can see, even though we experiment on such a small data set with barely no advanced training strategies nor hyper-parameter tuning, we could see a remarkable 3% AUC lift:"
      ],
      "metadata": {
        "id": "ScxHsPTNgYwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python kdd_2022_tutorial/main.py --task eval"
      ],
      "metadata": {
        "id": "02MlBLRTggMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}